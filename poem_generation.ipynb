{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "from docx.shared import Pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load up our text\n",
    "directory = os.getcwd()\n",
    "complete_text = []\n",
    "for filename in os.listdir(directory + '/data/'):\n",
    "    if filename.endswith(\".txt\"): \n",
    "        txt = open('data/' + filename, 'r').read().lower()\n",
    "        complete_text += [txt]\n",
    "\n",
    "text = '\\n'.join(complete_text)\n",
    "\n",
    "# extract all (unique) characters\n",
    "# these are our \"categories\" or \"labels\"\n",
    "chars = list(set(text))\n",
    "\n",
    "# set a fixed vector size\n",
    "# so we look at specific windows of characters\n",
    "max_len = 32\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our RNN. Keras makes this trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(max_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can generalize this like so:\n",
    "step = 3\n",
    "inputs = []\n",
    "outputs = []\n",
    "for i in range(0, len(text) - max_len, step):\n",
    "    inputs.append(text[i:i+max_len])\n",
    "    outputs.append(text[i+max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We also need to map each character to a label and create a reverse mapping to use later:\n",
    "char_labels = {ch:i for i, ch in enumerate(chars)}\n",
    "labels_char = {i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load previous ones\n",
    "char_labels = json.load(open('data/char_labels.json', 'r'))\n",
    "labels_char = json.load(open('data/labels_char.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_char_dict(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    return char_to_int\n",
    "\n",
    "char_labels = make_char_dict(text)\n",
    "json.dump(char_labels, open('data/char_labels.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_char = {int(label): char \n",
    "               for char, label in char_labels.items()}\n",
    "json.dump(labels_char, open('data/labels_char.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_matrix(text):\n",
    "    assert len(text) == max_len\n",
    "    X = np.zeros((1, max_len, len(chars)), dtype=np.bool)\n",
    "    for i, char in enumerate(text):\n",
    "        if char in char_labels:\n",
    "            X[0, i, char_labels[char]] = 1\n",
    "    return X   \n",
    "    \n",
    "print(char_labels['t'])\n",
    "text_to_matrix('this is a testing input sentence').astype(int)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using bool to reduce memory usage\n",
    "X = np.zeros((len(inputs), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(inputs), len(chars)), dtype=np.bool)\n",
    "\n",
    "# set the appropriate indices to 1| in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_labels[char]] = 1\n",
    "    y[i, char_labels[outputs[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data, we can start training. Keras also makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more epochs is usually better, but training can be very slow if not on a GPU\n",
    "epochs = 10\n",
    "model.fit(X, y, batch_size=128, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('data/rnn_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading pre-trained weights\n",
    "model.load_weights('data/rnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(temperature=0.35, max_len=32, seed=None, predicate=lambda x: len(x) < 100):\n",
    "    if seed is not None and len(seed) < max_len:\n",
    "        raise Exception('Seed text must be at least {} chars long'.format(max_len))\n",
    "    else:\n",
    "        start_idx = random.randint(0, len(text) - max_len - 1)\n",
    "        seed = text[start_idx:start_idx + max_len]\n",
    "\n",
    "    sentence = seed\n",
    "    generated = sentence\n",
    "    while predicate(generated):\n",
    "        X = text_to_matrix(sentence)\n",
    "\n",
    "        probs = model.predict(X, verbose=0)[0]\n",
    "\n",
    "        next_idx = sample(probs, temperature)\n",
    "        next_char = labels_char[next_idx]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    \n",
    "    if len(generated.split()[-1]) < 3:\n",
    "        generated_comp = generated.split('\\n')\n",
    "        generated = '\\n'.join(generated_comp[-1])\n",
    "    return generated\n",
    "\n",
    "def sample(probs, temperature):\n",
    "    \"\"\"samples an index from a vector of probabilities\"\"\"\n",
    "    a = np.log(probs)/temperature\n",
    "    a = np.exp(a)/np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = 'when the shoe fits, the foot is '\n",
    "X = text_to_matrix(input)\n",
    "probs = model.predict(X, verbose=0)[0]\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see the probability of an individual character\n",
    "label = char_labels['f']\n",
    "probs[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.argmax(probs)\n",
    "labels_char[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_ = generate(temperature=0.5,max_len=800)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanText(text_):\n",
    "    text_ = text_.strip()\n",
    "    new_text = []\n",
    "    for t in text_.split('\\n'):\n",
    "        if len(t) > 2:\n",
    "            new_text += [t[0].upper() + t[1:]]\n",
    "    text_ = '\\n'.join(new_text)\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_text = []\n",
    "for i in range(0,2000):\n",
    "    gen_text = generate(temperature=0.5,max_len=randint(800, 1000))\n",
    "    if len(gen_text) > 100:\n",
    "        all_text.append(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_text = [[cleanText(t)] for t in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_text, columns=['text'])\n",
    "df.to_csv('all_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_text = list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_poems_to_doc(all_text, single_files=True):\n",
    "    if not single_files:\n",
    "        all_ = '\\n\\n'.join([t for t in  all_text[:100] if isinstance(t, str)])\n",
    "    for i, text in enumerate(all_text):\n",
    "        if isinstance(text, str):\n",
    "            document = Document()\n",
    "            run = document.add_paragraph(text).add_run()\n",
    "            style = document.styles['Normal']\n",
    "            font = style.font\n",
    "            font.name = 'Big Calson' \n",
    "            font.size = Pt(12)\n",
    "            if single_files:\n",
    "                 document.save('data/output/poem_%s.docx' % i)\n",
    "            else:\n",
    "                document.save('data/output/ALL_POEMS.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_poems_to_doc(all_text)\n",
    "convert_poems_to_doc(all_text, single_files=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
